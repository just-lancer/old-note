package spark_supplement

object supplement03_persist {
  // RDD持久化
  /*
   1、为什么需要持久化
    RDD的执行过程中，RDD本身并不保存数据，当一个RDD或一部分（或系列）RDD的执行结果在后续过程中还被被使用时，
    如果不对这些RDD计算后的数据进行持久化，那么由于RDD只是保存了RDD的血缘关系，并不保存数据，所以，后续RDD的执行
    仍然是从最开始的RDD开始进行计算，这样就会做重复计算。因此当所需要一个RDD的计算数据需要被重复使用时，可以进行
    数据的持久化
   */

  /*
    2、如何进行持久化
      方法一：RDD Cash缓存，调用cash()方法
          RDD通过Cache或者Persist方法将前面的计算结果缓存，默认情况下会把数据以缓存在JVM的堆内存中。
          但是并不是这两个方法被调用时立即缓存，而是触发后面的行动算子时，
          该RDD的计算数据将会被缓存在计算节点的内存中，并供后面重用。

      方法二：RDD Persist缓存，调用persist()方法
          persist()方法是cash()方法的底层实现方法，cash()方法默认是将RDD的数据缓存到内存中
          而persist()方法能对RDD数据设置缓存级别，通过传入参数

      方法三：RDD CheckPoint检查点
          所谓的检查点其实就是通过将RDD中间结果写入磁盘

          由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，
          如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。

          对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。

          设置检查点，需要指定数据保存的位置，可以是本地文件中，也可以是HDFS文件系统中
          利用spark连接对象调用setCheckPointDir()方法设置检查点文件路径：

          对于需要进行数据持久化的RDD，利用该RDD对象调用checkPoint()方法即可将数据持久化到指定的文件

   */

  /*
    3、持久化的优化
      对于使用检查点进行RDD数据持久化时，它并不是直接将RDD数据进行持久化，而是会将该RDD之前的所有RDD都再执行
      一遍，然后将得到的结果进行持久化，因此存在重复计算的问题。

      优化方案：Cash缓存和检查点结合使用。
        在需要进行检查点持久化的RDD处，先将RDD进行Cash持久化，再进行检查点持久化

        先进行Cash持久化，相当于将该RDD之前的血缘关系切断了，那么检查点再进行持久化时，直接可以将该数据进行
        持久化，而不会重复执行之前的RDD
   */

  /*
    4、缓存和检查点区别
        Cache缓存只是将数据保存起来，不切断血缘依赖。Checkpoint检查点切断血缘依赖。
        Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint的数据通常存储在HDFS等容错、高可用的文件系统，可靠性高。
        建议对checkpoint()的RDD使用Cache缓存，这样checkpoint的job只需从Cache缓存中读取数据即可，否则需要再从头计算一次RDD。

   */


}
